# -*- coding: utf-8 -*-
"""ML_LAB_W10_KNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eNw6mZ-QfZR3pBGP3cL5Z9W72kefK3tf
"""

import numpy as np
from sklearn.datasets import load_iris
iris = load_iris()

iris.feature_names

iris.target_names

from sklearn.model_selection import train_test_split
import pandas as pd
df = pd.DataFrame(iris.data,columns=iris.feature_names)
df.head()

df['target'] = iris.target
df.head()

df['flower_name'] =df.target.apply(lambda x: iris.target_names[x])
df.head()

df0 = df[:50]
df1=df[50:100]
df2=df[100:]

import matplotlib.pyplot as plt
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.scatter(df0['sepal length (cm)'],df0['sepal width (cm)'],marker='*',color='blue')
plt.scatter(df1['sepal length (cm)'],df1['sepal width (cm)'],marker='+',color='red')
plt.show()

plt.xlabel('Petal length')
plt.ylabel('Petal width')
plt.scatter(df0['petal length (cm)'],df0['petal width (cm)'],marker='*',color='blue')
plt.scatter(df1['petal length (cm)'],df1['petal width (cm)'],marker='+',color='green')
plt.show()

plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.scatter(df1['sepal length (cm)'],df1['sepal width (cm)'],marker='*',color='blue')
plt.scatter(df2['sepal length (cm)'],df2['sepal width (cm)'],marker='+',color='red')
plt.show()

plt.xlabel('Petal length')
plt.ylabel('Petal width')
plt.scatter(df1['petal length (cm)'],df1['petal width (cm)'],marker='*',color='blue')
plt.scatter(df2['petal length (cm)'],df2['petal width (cm)'],marker='+',color='green')
plt.show()

X = df.drop(['target','flower_name'],axis=1)
y = df['target']
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)
print(X_train.shape,X_test.shape)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=10,metric='euclidean')

knn.fit(X_train,y_train)

knn.score(X_test,y_test)

flower = {0:'Iris-setosa',2:'Iris-virginica',1:'Iris-Versicolor'}
flower[knn.predict([[5.2,3.1,1.4,0.2]])[0]]

from sklearn.neighbors import KNeighborsClassifier
knn_temp = KNeighborsClassifier(n_neighbors=5,metric='euclidean')
knn_temp.fit(X_train,y_train)
knn_temp.score(X_test,y_test)

from sklearn.neighbors import KNeighborsClassifier
knn_temp = KNeighborsClassifier(n_neighbors=7,metric='manhattan')
knn_temp.fit(X_train,y_train)
knn_temp.score(X_test,y_test)

from sklearn.neighbors import KNeighborsClassifier
knn_temp = KNeighborsClassifier(n_neighbors=3,metric='minkowski')
knn_temp.fit(X_train,y_train)
knn_temp.score(X_test,y_test)

"""### Question 1"""

pdf = pd.read_csv('/content/Data.csv')
pdf.head()

pdf.Risk.unique()

pdf.head(9)

"""import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from scipy.spatial.distance import euclidean

# Load the dataset
data = pd.read_csv('/content/Data.csv')

# Extract the feature matrix and the target vector
X = data[['Age', 'Income']].values
y = data['Risk'].values
data['Income'] = data['Income'].str.replace('$', '')
"""
# Standardize the feature matrix using min-max standardization
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Add the new record to the feature matrix
new_record = [[66, 1, 36120.34]]
X_scaled = np.concatenate((X_scaled, new_record))

# Perform k-NN classification
k = 9
distances = []
for i in range(len(X_scaled)):
    if i != 9:  # Exclude the new record from the distance calculation
        distance = euclidean(X_scaled[9], X_scaled[i])
        distances.append((i, distance))

# Sort the distances and get the indices of the k-closest records
distances.sort(key=lambda x: x[1])
k_indices = [d[0] for d in distances[:k]]

# Perform unweighted voting
votes = {'Bad loss': 0, 'Good risk': 1}
for i in k_indices:
    votes[y[i]] += 1

# Classify the new record based on the majority vote
new_record_risk = max(votes, key=votes.get)

print(f'The risk factor for the new record is: {new_record_risk}')

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from scipy.spatial.distance import euclidean

# Load the dataset
data = pd.read_csv('/content/Data.csv')

# Extract the feature matrix and the target vector

data['Income'] = data['Income'].str.replace('$', '').str.replace(',', '').astype(float)

#print(df)
data.head()

data['Marital'] = data['Marital'].map({'Single':0,'Married':1,'Other':2})
X = data[['Age', 'Marital','Income']].values
y = data['Risk'].values

data.head(10)

X = data[['Age', 'Marital','Income']][:9].values
y = data['Risk'][:9].values

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=9,metric='euclidean')
knn1 = KNeighborsClassifier(n_neighbors=9,metric='manhattan')
knn2=KNeighborsClassifier(n_neighbors=9,metric='minkowski')
knn.fit(X,y)
knn1.fit(X,y)
knn2.fit(X,y)

risk = {0:'Bad Loss',1:'Good Risk'}
knn.predict([[66, 1, 36120.34]])

knn1.predict([[66, 1, 36120.34]])

knn2.predict([[66, 1, 36120.34]])

data.head()

